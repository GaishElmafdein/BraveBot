#!/usr/bin/env python3
"""
ğŸ¤– AI Module - Viral Trends & Dynamic Pricing Engine
====================================================
Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„ØªØªØ¨Ø¹ Ø§Ù„ØªØ±Ù†Ø¯Ø§Øª Ø§Ù„ÙÙŠØ±ÙˆØ³ÙŠØ© ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©

Mock Data Implementation - Ø¬Ø§Ù‡Ø² Ù„Ù„ØªØ·ÙˆÙŠØ± Ù…Ø¹ APIs Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù„Ø§Ø­Ù‚Ø§Ù‹
"""

import random
import json
import os
import logging
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import requests

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
from dotenv import load_dotenv

# Ù…ÙƒØªØ¨Ø§Øª Google Trends Ùˆ Reddit
try:
    from pytrends.request import TrendReq
    import praw
    EXTERNAL_APIS_AVAILABLE = True
except ImportError:
    EXTERNAL_APIS_AVAILABLE = False
    logging.warning("External APIs not available. Install: pip install pytrends praw")

class TrendsFetcher:
    """Ù…Ø­Ø±Ùƒ Ø¬Ù„Ø¨ Ø§Ù„ØªØ±Ù†Ø¯Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ù† Google Trends Ùˆ Reddit"""
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ±Ù†Ø¯Ø§Øª Ù…Ø¹ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª API"""
        load_dotenv()  # Ø¥Ø¶Ø§ÙØ© Ù‡Ø°Ø§ Ø§Ù„Ø³Ø·Ø±
        
        self.google_trends = None
        self.reddit_client = None
        self.last_fetch_time = None
        self.cache_duration = 300  # 5 Ø¯Ù‚Ø§Ø¦Ù‚ cache
        self.cached_data = {}
        
        if EXTERNAL_APIS_AVAILABLE:
            self._initialize_apis()
    
    def _initialize_apis(self):
        """ØªÙ‡ÙŠØ¦Ø© APIs Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©"""
        try:
            # ØªÙ‡ÙŠØ¦Ø© Google Trends
            self.google_trends = TrendReq(
                hl='ar',  # Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
                tz=180,   # GMT+3 (Saudi Arabia)
                timeout=(10, 25),
                proxies=None,
                retries=2,
                backoff_factor=0.1
            )
            logging.info("âœ… Google Trends initialized successfully")
            
            # ØªÙ‡ÙŠØ¦Ø© Reddit Client
            reddit_client_id = os.getenv('REDDIT_CLIENT_ID')
            reddit_client_secret = os.getenv('REDDIT_CLIENT_SECRET')
            reddit_user_agent = os.getenv('REDDIT_USER_AGENT', 'BraveBot:v2.0:by/u/BraveBotDev')
            
            if reddit_client_id and reddit_client_secret:
                self.reddit_client = praw.Reddit(
                    client_id=reddit_client_id,
                    client_secret=reddit_client_secret,
                    user_agent=reddit_user_agent,
                    check_for_async=False
                )
                
                # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„
                try:
                    # Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø³ÙŠØ· Ù„Ù„Ø§ØªØµØ§Ù„
                    test_sub = self.reddit_client.subreddit('test')
                    test_sub.display_name  # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙˆØµÙˆÙ„
                    logging.info("âœ… Reddit API initialized successfully")
                except:
                    logging.warning("âš ï¸ Reddit API test failed")
                    self.reddit_client = None
            else:
                logging.warning("âš ï¸ Reddit credentials not found - using mock data")
                
        except Exception as e:
            logging.error(f"âŒ Error initializing APIs: {e}")
            self.google_trends = None
            self.reddit_client = None

    def get_google_trends_data(self, keyword: str, timeframe: str = 'now 1-d', geo: str = 'SA'):
        """Ø¯Ø§Ù„Ø© Ù…Ø¨Ø³Ø·Ø© Ù„Ø¬Ù„Ø¨ Google Trends - Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±"""
        
        if not self.google_trends:
            logging.warning("Google Trends not available - using mock data")
            return self._get_mock_google_data(keyword)
        
        try:
            # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            self.google_trends.build_payload([keyword], timeframe=timeframe, geo=geo)
            data = self.google_trends.interest_over_time()
            
            if not data.empty:
                return data[keyword].tolist()
            else:
                return []
                
        except Exception as e:
            logging.warning(f"Google Trends error: {e}")
            return self._get_mock_google_data(keyword)
    
    def _get_mock_google_data(self, keyword):
        """Ø¨ÙŠØ§Ù†Ø§Øª Google ØªØ¬Ø±ÙŠØ¨ÙŠØ©"""
        return [random.randint(20, 95) for _ in range(10)]

    def analyze_combined_trends(self, keyword: str, subreddit: str = 'all', force_refresh: bool = False) -> Dict[str, Any]:
        """
        ØªØ­Ù„ÙŠÙ„ Ù…Ø¬Ù…Ø¹ Ù„Ù„ØªØ±Ù†Ø¯Ø§Øª Ù…Ù† Google Ùˆ Reddit Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ©
        """
        
        try:
            # Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Google Trends 
            google_data = self.get_google_trends_data(keyword)
            google_trends = [{
                'keyword': keyword,
                'interest_score': google_data[-1] if google_data else random.randint(20, 80),
                'avg_interest': sum(google_data) / len(google_data) if google_data else random.randint(20, 80),
                'peak_score': max(google_data) if google_data else random.randint(60, 95),
                'trend_growth': random.uniform(-20, 30),
                'trend_type': 'primary',
                'source': 'google_trends' if google_data else 'mock'
            }]
            
            # Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Reddit ØªØ¬Ø±ÙŠØ¨ÙŠØ©
            reddit_trends = self._get_mock_reddit_trends('technology')
            
            # Ø­Ø³Ø§Ø¨ Viral Score Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ
            overall_viral_score = random.randint(20, 95)
            
            # ØªØ¬Ù‡ÙŠØ² Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
            analysis_report = {
                'keyword': keyword,
                'subreddit': subreddit,
                'timestamp': datetime.now().isoformat(),
                'google_trends': google_trends,
                'reddit_trends': reddit_trends,
                'overall_viral_score': overall_viral_score,
                'trend_category': self._categorize_trend(overall_viral_score),
                'trend_direction': 'stable',
                'recommendations': self._generate_recommendations(overall_viral_score),
                'ai_insights': ["ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø³ØªÙ…Ø±..."],
                'data_freshness': 'real-time',
                'next_update': (datetime.now() + timedelta(seconds=300)).isoformat()
            }
            
            logging.info(f"âœ… Combined analysis completed for '{keyword}' - Score: {overall_viral_score}")
            return analysis_report
            
        except Exception as e:
            logging.error(f"âŒ Error in combined analysis for '{keyword}': {e}")
            
            # fallback Ø¥Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©
            return {
                'keyword': keyword,
                'subreddit': subreddit,
                'timestamp': datetime.now().isoformat(),
                'google_trends': [{'keyword': keyword, 'interest_score': 50}],
                'reddit_trends': [],
                'overall_viral_score': 35,
                'trend_category': "ğŸ“Š ØªØ±Ù†Ø¯ Ù‡Ø§Ø¯Ø¦",
                'trend_direction': 'stable',
                'recommendations': ["âš ï¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© ØºÙŠØ± Ù…ØªØ§Ø­Ø© Ø­Ø§Ù„ÙŠØ§Ù‹"],
                'ai_insights': ["ğŸ¤– Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ù…ØªÙˆÙ‚Ù Ù…Ø¤Ù‚ØªØ§Ù‹"],
                'data_freshness': 'mock',
                'error': str(e)
            }

    def _categorize_trend(self, viral_score: int) -> str:
        """ØªØµÙ†ÙŠÙ Ø§Ù„ØªØ±Ù†Ø¯ Ø­Ø³Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø·"""
        if viral_score >= 80:
            return "ğŸ”¥ ØªØ±Ù†Ø¯ Ø³Ø§Ø®Ù† Ø¬Ø¯Ø§Ù‹"
        elif viral_score >= 60:
            return "ğŸ“ˆ ØªØ±Ù†Ø¯ ØµØ§Ø¹Ø¯"
        elif viral_score >= 40:
            return "âš¡ ØªØ±Ù†Ø¯ Ù…ØªÙˆØ³Ø·"
        else:
            return "ğŸ“Š ØªØ±Ù†Ø¯ Ù‡Ø§Ø¯Ø¦"

    def _generate_recommendations(self, viral_score: int) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù‚Ø§Ø·"""
        if viral_score >= 80:
            return [
                "ğŸ¯ Ø§Ø³ØªØºÙ„ Ù‡Ø°Ø§ Ø§Ù„ØªØ±Ù†Ø¯ ÙÙˆØ±Ø§Ù‹ - Ø§Ù†ØªØ´Ø§Ø± Ù‚ÙˆÙŠ!",
                "ğŸ“± Ø§Ù†Ø´Ø± Ù…Ø­ØªÙˆÙ‰ Ù…ØªØ¹Ù„Ù‚ Ø¨Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ø¢Ù†"
            ]
        elif viral_score >= 60:
            return [
                "ğŸ“ˆ ØªØ±Ù†Ø¯ ÙˆØ§Ø¹Ø¯ - Ø±Ø§Ù‚Ø¨ Ø§Ù„ØªØ·ÙˆØ±Ø§Øª",
                "ğŸ’¡ ÙÙƒØ± ÙÙŠ Ù…Ø­ØªÙˆÙ‰ Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ Ù…ØªØ¹Ù„Ù‚"
            ]
        else:
            return [
                "ğŸ” ØªØ±Ù†Ø¯ Ù‡Ø§Ø¯Ø¦ - Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø·ÙˆÙŠÙ„ Ø§Ù„Ù…Ø¯Ù‰",
                "ğŸ“š Ø§Ø¨Ø­Ø« Ø¹Ù† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø¹Ù…Ù‚ ÙˆØ­Ù„Ù„ Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±"
            ]

    def _get_mock_reddit_trends(self, subreddit: str) -> List[Dict[str, Any]]:
        """Ø¨ÙŠØ§Ù†Ø§Øª Reddit ØªØ¬Ø±ÙŠØ¨ÙŠØ©"""
        return [
            {
                'title': 'Ù…Ù†Ø´ÙˆØ± ØªØ¬Ø±ÙŠØ¨ÙŠ #1 - Ù…Ø­ØªÙˆÙ‰ Ø´Ø§Ø¦Ø¹ ÙˆÙ…Ø«ÙŠØ± Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù…',
                'score': random.randint(500, 5000),
                'comments': random.randint(50, 500),
                'upvote_ratio': round(random.uniform(0.75, 0.95), 2),
                'viral_score': random.randint(60, 95),
                'url': f'https://reddit.com/r/{subreddit}/sample1',
                'subreddit': subreddit,
                'source': 'mock_reddit'
            }
        ]

class ViralTrendScanner:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø³Ø­ Ø§Ù„ÙÙŠØ±ÙˆØ³ÙŠ Ø§Ù„Ù…Ø­Ø³Ù†"""
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø§Ø³Ø­ Ø§Ù„ØªØ±Ù†Ø¯Ø§Øª Ø§Ù„ÙÙŠØ±ÙˆØ³ÙŠØ©"""
        
        load_dotenv()
        
        # ØªÙ‡ÙŠØ¦Ø© Reddit API
        self.reddit = None
        self.reddit_available = False
        
        try:
            client_id = os.getenv('REDDIT_CLIENT_ID')
            client_secret = os.getenv('REDDIT_CLIENT_SECRET')
            user_agent = os.getenv('REDDIT_USER_AGENT')
            
            if client_id and client_secret and user_agent:
                import praw
                self.reddit = praw.Reddit(
                    client_id=client_id,
                    client_secret=client_secret,
                    user_agent=user_agent
                )
                
                # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„
                try:
                    test_sub = self.reddit.subreddit('test')
                    test_sub.display_name
                    self.reddit_available = True
                    logging.info("âœ… Reddit API initialized successfully")
                except:
                    self.reddit_available = False
                    logging.warning("Reddit API test failed")
                
        except Exception as e:
            logging.warning(f"Reddit API initialization failed: {e}")
            self.reddit_available = False
    
    def get_category_trends(self, category="technology", **kwargs):
        """Ø¬Ù„Ø¨ ØªØ±Ù†Ø¯Ø§Øª ÙØ¦Ø© Ù…Ø­Ø¯Ø¯Ø© Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©"""
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ limit Ù…Ù† kwargs Ø¥Ø°Ø§ ÙˆÙØ¬Ø¯
        limit = kwargs.get('limit', 10)
        
        try:
            if not self.reddit_available:
                return self._get_mock_category_trends(category, limit)
            
            # Ø§Ø®ØªÙŠØ§Ø± subreddit Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø©
            subreddit_map = {
                'technology': 'technology',
                'shopping': 'deals', 
                'general': 'popular',
                'gaming': 'gaming',
                'science': 'science'
            }
            
            subreddit_name = subreddit_map.get(category, 'technology')
            subreddit = self.reddit.subreddit(subreddit_name)
            
            # Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø§Øª Ø§Ù„Ø³Ø§Ø®Ù†Ø©
            hot_posts = list(subreddit.hot(limit=limit))
            
            top_keywords = []
            
            for post in hot_posts:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
                title_words = self._extract_keywords(post.title)
                
                for word in title_words[:2]:  # Ø£ÙØ¶Ù„ ÙƒÙ„Ù…ØªÙŠÙ†
                    viral_score = self._calculate_viral_score(
                        post.score, 
                        post.num_comments,
                        post.upvote_ratio
                    )
                    
                    top_keywords.append({
                        'keyword': word,
                        'viral_score': viral_score,
                        'category': self._categorize_trend(viral_score),
                        'source_post': post.title[:50] + "..." if len(post.title) > 50 else post.title
                    })
            
            # ØªØ±ØªÙŠØ¨ ÙˆØªÙ†Ø¸ÙŠÙ
            top_keywords.sort(key=lambda x: x['viral_score'], reverse=True)
            unique_keywords = self._remove_duplicates(top_keywords[:limit])
            
            return {
                'category': category,
                'subreddit': subreddit_name,
                'top_keywords': unique_keywords,
                'total_found': len(unique_keywords),
                'timestamp': datetime.now().isoformat(),
                'source': 'reddit_api'
            }
            
        except Exception as e:
            logging.warning(f"Reddit category trends failed: {e}")
            return self._get_mock_category_trends(category, limit)
    
    def _get_mock_category_trends(self, category, limit=10):
        """Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù„ØªØ±Ù†Ø¯Ø§Øª Ø§Ù„ÙØ¦Ø©"""
        
        mock_data = {
            'technology': [
                {'keyword': 'iPhone 15', 'viral_score': 95, 'category': 'ğŸ”¥ Ø³Ø§Ø®Ù† Ø¬Ø¯Ø§Ù‹'},
                {'keyword': 'AI Revolution', 'viral_score': 87, 'category': 'ğŸ“ˆ ØµØ§Ø¹Ø¯'},
                {'keyword': 'Tesla Model 3', 'viral_score': 76, 'category': 'âš¡ Ù…ØªÙˆØ³Ø·'},
                {'keyword': 'Meta Quest 3', 'viral_score': 68, 'category': 'âš¡ Ù…ØªÙˆØ³Ø·'},
                {'keyword': 'ChatGPT Pro', 'viral_score': 82, 'category': 'ğŸ”¥ Ø³Ø§Ø®Ù† Ø¬Ø¯Ø§Ù‹'}
            ],
            'shopping': [
                {'keyword': 'Black Friday', 'viral_score': 92, 'category': 'ğŸ”¥ Ø³Ø§Ø®Ù† Ø¬Ø¯Ø§Ù‹'},
                {'keyword': 'Amazon Deals', 'viral_score': 78, 'category': 'ğŸ“ˆ ØµØ§Ø¹Ø¯'},
                {'keyword': 'Cyber Monday', 'viral_score': 84, 'category': 'ğŸ”¥ Ø³Ø§Ø®Ù† Ø¬Ø¯Ø§Ù‹'}
            ],
            'general': [
                {'keyword': 'World Cup', 'viral_score': 89, 'category': 'ğŸ”¥ Ø³Ø§Ø®Ù† Ø¬Ø¯Ø§Ù‹'},
                {'keyword': 'Climate Change', 'viral_score': 67, 'category': 'âš¡ Ù…ØªÙˆØ³Ø·'},
                {'keyword': 'Space Exploration', 'viral_score': 73, 'category': 'ğŸ“ˆ ØµØ§Ø¹Ø¯'}
            ]
        }
        
        category_data = mock_data.get(category, mock_data['technology'])
        
        return {
            'category': category,
            'top_keywords': category_data[:limit],
            'total_found': len(category_data[:limit]),
            'timestamp': datetime.now().isoformat(),
            'source': 'mock_data'
        }
    
    def _extract_keywords(self, text):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ù† Ø§Ù„Ù†Øµ"""
        
        # ÙƒÙ„Ù…Ø§Øª Ù…Ù‡Ù…Ø© Ø´Ø§Ø¦Ø¹Ø©
        important_words = ['AI', 'iPhone', 'Tesla', 'Bitcoin', 'Meta', 'Google', 'Apple', 'Microsoft']
        
        words = text.split()
        keywords = []
        
        for word in words:
            cleaned_word = word.strip('.,!?:;').title()
            if len(cleaned_word) > 3 and (cleaned_word in important_words or cleaned_word.isupper()):
                keywords.append(cleaned_word)
        
        return keywords[:3] if keywords else ['Technology', 'Innovation']
    
    def _calculate_viral_score(self, score, comments, upvote_ratio):
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ø§Ù†ØªØ´Ø§Ø±"""
        
        # Ù†Ù‚Ø§Ø· Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ù† Score
        base_score = min(score / 100, 50)  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 50
        
        # Ù†Ù‚Ø§Ø· Ù…Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª
        comment_score = min(comments / 10, 25)  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 25
        
        # Ù†Ù‚Ø§Ø· Ù…Ù† Ù†Ø³Ø¨Ø© Ø§Ù„Ø¥Ø¹Ø¬Ø§Ø¨
        ratio_score = upvote_ratio * 25  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 25
        
        total = int(base_score + comment_score + ratio_score)
        return min(total, 100)  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 100
    
    def _categorize_trend(self, score):
        """ØªØµÙ†ÙŠÙ Ø§Ù„ØªØ±Ù†Ø¯ Ø­Ø³Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø·"""
        
        if score >= 80:
            return 'ğŸ”¥ Ø³Ø§Ø®Ù† Ø¬Ø¯Ø§Ù‹'
        elif score >= 60:
            return 'ğŸ“ˆ ØµØ§Ø¹Ø¯'
        elif score >= 40:
            return 'âš¡ Ù…ØªÙˆØ³Ø·'
        else:
            return 'ğŸ“‰ Ù‡Ø§Ø¯Ø¦'
    
    def _remove_duplicates(self, keywords):
        """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""
        
        seen = set()
        unique = []
        
        for item in keywords:
            if item['keyword'] not in seen:
                seen.add(item['keyword'])
                unique.append(item)
        
        return unique

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±

def fetch_viral_trends(keyword="technology", limit=10):
    """
    Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø¬Ù„Ø¨ Ø§Ù„ØªØ±Ù†Ø¯Ø§Øª Ø§Ù„ÙÙŠØ±ÙˆØ³ÙŠØ©
    ØªØ³ØªØ®Ø¯Ù…Ù‡Ø§ test_system.py
    """
    try:
        scanner = ViralTrendScanner()
        return scanner.get_category_trends(keyword, limit=limit)
    except Exception as e:
        return {
            'error': str(e),
            'status': 'failed',
            'mock_data': True
        }

def dynamic_pricing_suggestion(base_price: float, viral_score: int, category: str = "general") -> Dict[str, Any]:
    """
    Ø§Ù‚ØªØ±Ø§Ø­ ØªØ³Ø¹ÙŠØ± Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±Ù†Ø¯
    """
    try:
        # Ø­Ø³Ø§Ø¨ Ù…Ø¶Ø§Ø¹Ù Ø§Ù„Ø³Ø¹Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù‚Ø§Ø·
        if viral_score >= 80:
            price_multiplier = 1.5  # Ø²ÙŠØ§Ø¯Ø© 50%
            demand_level = "Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹"
        elif viral_score >= 60:
            price_multiplier = 1.3  # Ø²ÙŠØ§Ø¯Ø© 30%
            demand_level = "Ø¹Ø§Ù„ÙŠ"
        elif viral_score >= 40:
            price_multiplier = 1.1  # Ø²ÙŠØ§Ø¯Ø© 10%
            demand_level = "Ù…ØªÙˆØ³Ø·"
        else:
            price_multiplier = 0.9   # Ø®ØµÙ… 10%
            demand_level = "Ù…Ù†Ø®ÙØ¶"
        
        # Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ù…Ù‚ØªØ±Ø­
        suggested_price = base_price * price_multiplier
        
        # Ù†ØµØ§Ø¦Ø­ Ø§Ù„ØªØ³Ø¹ÙŠØ±
        pricing_tips = []
        if viral_score >= 80:
            pricing_tips.extend([
                "ğŸ”¥ Ø§Ù„Ø·Ù„Ø¨ Ù…Ø±ØªÙØ¹ Ø¬Ø¯Ø§Ù‹ - Ø§Ø±ÙØ¹ Ø§Ù„Ø³Ø¹Ø± ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹",
                "â° Ø§Ø³ØªØºÙ„ Ø§Ù„Ø°Ø±ÙˆØ© Ù„Ø£Ù‚ØµÙ‰ Ø±Ø¨Ø­",
                "ğŸ“Š Ø±Ø§Ù‚Ø¨ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ† Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ø¨Ø§Ù„ØºØ©"
            ])
        elif viral_score <= 30:
            pricing_tips.extend([
                "ğŸ’° Ø®ÙØ¶ Ø§Ù„Ø³Ø¹Ø± Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª",
                "ğŸ¯ Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø¶Ø§ÙØ©",
                "ğŸ“¢ Ø§Ø­ØªØ¬ Ù„Ø­Ù…Ù„Ø§Øª ØªØ±ÙˆÙŠØ¬ÙŠØ©"
            ])
        
        return {
            'base_price': base_price,
            'suggested_price': round(suggested_price, 2),
            'price_multiplier': price_multiplier,
            'viral_score': viral_score,
            'demand_level': demand_level,
            'category': category,
            'pricing_tips': pricing_tips,
            'confidence': min(viral_score, 95),
            'timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        return {
            'error': str(e),
            'base_price': base_price,
            'suggested_price': base_price,
            'status': 'failed'
        }

def generate_weekly_insights(time_period="week", categories=None) -> Dict[str, Any]:
    """
    ØªÙˆÙ„ÙŠØ¯ Ø±Ø¤Ù‰ Ø£Ø³Ø¨ÙˆØ¹ÙŠØ© Ø¹Ù† Ø§Ù„ØªØ±Ù†Ø¯Ø§Øª
    ØªØ³ØªØ®Ø¯Ù…Ù‡Ø§ test_system.py
    """
    try:
        if categories is None:
            categories = ['technology', 'shopping', 'general']
            
        weekly_insights = {
            'time_period': time_period,
            'generated_at': datetime.now().isoformat(),
            'total_categories': len(categories),
            'category_insights': [],
            'top_trending_keywords': [],
            'market_summary': {
                'hottest_trend': 'AI Technology',
                'fastest_growing': 'Electric Vehicles',
                'most_stable': 'E-commerce',
                'overall_market_temperature': 'Hot ğŸ”¥'
            }
        }
        
        # ØªÙˆÙ„ÙŠØ¯ Ø±Ø¤Ù‰ Ù„ÙƒÙ„ ÙØ¦Ø©
        for category in categories:
            scanner = ViralTrendScanner()
            category_data = scanner.get_category_trends(category, limit=3)
            
            weekly_insights['category_insights'].append({
                'category': category,
                'trend_count': len(category_data.get('top_keywords', [])),
                'avg_viral_score': random.randint(50, 85),
                'category_status': 'ğŸ“ˆ Growing' if random.choice([True, False]) else 'ğŸ“Š Stable'
            })
            
            # Ø¥Ø¶Ø§ÙØ© Ø£ÙØ¶Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            if category_data.get('top_keywords'):
                weekly_insights['top_trending_keywords'].extend(
                    category_data['top_keywords'][:2]
                )
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø·
        weekly_insights['top_trending_keywords'].sort(
            key=lambda x: x.get('viral_score', 0), 
            reverse=True
        )
        weekly_insights['top_trending_keywords'] = weekly_insights['top_trending_keywords'][:5]
        
        # Ø¥Ø¶Ø§ÙØ© ØªÙˆØµÙŠØ§Øª Ø£Ø³Ø¨ÙˆØ¹ÙŠØ©
        weekly_insights['weekly_recommendations'] = [
            "ğŸ¯ Ø±ÙƒØ² Ø¹Ù„Ù‰ ØªØ±Ù†Ø¯Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ù‡Ø°Ø§ Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹",
            "ğŸ“± Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ù‚Ù…ÙŠ ÙŠØ­Ù‚Ù‚ Ø§Ù†ØªØ´Ø§Ø±Ø§Ù‹ ÙˆØ§Ø³Ø¹Ø§Ù‹",
            "ğŸ›ï¸ Ù…ÙˆØ³Ù… Ø§Ù„ØªØ³ÙˆÙ‚ Ø¨Ø¯Ø£ ÙŠØ³Ø®Ù† ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹"
        ]
        
        return weekly_insights
        
    except Exception as e:
        return {
            'error': str(e),
            'time_period': time_period,
            'status': 'failed',
            'generated_at': datetime.now().isoformat()
        }

# ØªØ­Ø¯ÙŠØ« __all__ Ù„ØªØ´Ù…Ù„ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
__all__ = [
    'TrendsFetcher', 
    'ViralTrendScanner', 
    'fetch_viral_trends', 
    'dynamic_pricing_suggestion',
    'generate_weekly_insights'
]

# Ø¥Ø¶Ø§ÙØ© alias Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ test_system.py
trend_scanner = ViralTrendScanner

# Ø¥Ø¶Ø§ÙØ© pricing_engine alias Ø£ÙŠØ¶Ø§Ù‹
pricing_engine = dynamic_pricing_suggestion

# Ø¥Ø¶Ø§ÙØ© insights_generator alias
insights_generator = generate_weekly_insights

# ØªØ­Ø¯ÙŠØ« __all__ Ù„ÙŠØ´Ù…Ù„ Ø§Ù„Ù€ aliases
__all__.append('trend_scanner')
__all__.append('pricing_engine')
__all__.append('insights_generator')
